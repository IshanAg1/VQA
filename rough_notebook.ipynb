{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5060663,"sourceType":"datasetVersion","datasetId":2938312},{"sourceId":14330200,"sourceType":"datasetVersion","datasetId":9148806},{"sourceId":14334423,"sourceType":"datasetVersion","datasetId":9151823},{"sourceId":143614612,"sourceType":"kernelVersion"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-29T09:41:48.415580Z","iopub.execute_input":"2025-12-29T09:41:48.415836Z","iopub.status.idle":"2025-12-29T09:42:25.443854Z","shell.execute_reply.started":"2025-12-29T09:41:48.415806Z","shell.execute_reply":"2025-12-29T09:42:25.442732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import torch\n# from PIL import Image\n# from torch.utils.data import Dataset, DataLoader\n# from transformers import BertTokenizer, BertModel, CLIPProcessor, CLIPVisionModel\n\n# # Paths of uploaded dataset files\n# IMG_DIR = \"/kaggle/input/mini-coco2014-dataset-for-image-captioning/Images\"\n# QUEST_PATH = \"/kaggle/input/vqatext/v2_Questions_Train_mscoco/v2_OpenEnded_mscoco_train2014_questions.json\"\n# ANNOT_PATH = \"/kaggle/input/vqatext/v2_Annotations_Train_mscoco/v2_mscoco_train2014_annotations.json\"\n\n# # using GPU\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Verified Device: {device}\")\n\n# # # Paths based on your uploaded Kaggle screenshot\n# # IMG_DIR = \"/kaggle/input/mini-coco2014-dataset-for-image-captioning/Images\"\n# # QUEST_PATH = \"/kaggle/input/vqatext/v2_Questions_Train_mscoco/v2_OpenEnded_mscoco_train2014_questions.json\"\n# # ANNOT_PATH = \"/kaggle/input/vqatext/v2_Annotations_Train_mscoco/v2_mscoco_train2014_annotations.json\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T13:26:34.285836Z","iopub.execute_input":"2025-12-29T13:26:34.286431Z","iopub.status.idle":"2025-12-29T13:26:34.292157Z","shell.execute_reply.started":"2025-12-29T13:26:34.286401Z","shell.execute_reply":"2025-12-29T13:26:34.291202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# class MiniVQADataset(Dataset):\n#     def __init__(self, questions_path, annotations_path, img_dir, tokenizer, processor):\n#         # 1. Load all data\n#         with open(questions_path, 'r') as f:\n#             all_questions = json.load(f)['questions']\n#         with open(annotations_path, 'r') as f:\n#             self.annotations = {a['question_id']: a for a in json.load(f)['annotations']}\n        \n#         self.img_dir = img_dir\n#         self.tokenizer = tokenizer\n#         self.processor = processor\n#         # self.ans_to_id = {\"yes\": 0, \"no\": 1, \"2\": 2, \"1\": 3, \"white\": 4}\n\n#         # --- NEW CODE STARTS HERE: Create a dynamic Answer Vocabulary ---\n#         # Get all answers from the annotations you actually have\n#         all_raw_answers = [a['multiple_choice_answer'] for a in self.annotations.values()]\n        \n#         # Get the 1000 most frequent answers (this is standard VQA research practice)\n#         most_common = Counter(all_raw_answers).most_common(1000)\n#         self.ans_to_id = {ans[0]: i for i, ans in enumerate(most_common)}\n        \n\n#         # 2. FILTERING LOGIC: Only keep questions if the image exists on disk\n#         self.valid_questions = []\n#         print(\"Filtering dataset for existing images... please wait.\")\n        \n#         for q in all_questions:\n#             img_id = str(q['image_id']).zfill(12)\n#             img_path = os.path.join(self.img_dir, f\"COCO_train2014_{img_id}.jpg\")\n            \n#             if os.path.exists(img_path):\n#                 self.valid_questions.append(q)\n            \n#             # Stop once we have 2000 valid samples to save time\n#             if len(self.valid_questions) >= 10000:\n#                 break\n                \n#         print(f\"Done! Found {len(self.valid_questions)} matching images in your mini dataset.\")\n\n#     def __len__(self):\n#         return len(self.valid_questions)\n\n#     def __getitem__(self, idx):\n#         q_data = self.valid_questions[idx]\n#         q_id = q_data['question_id']\n#         img_id = str(q_data['image_id']).zfill(12)\n        \n#         img_path = os.path.join(self.img_dir, f\"COCO_train2014_{img_id}.jpg\")\n#         image = Image.open(img_path).convert(\"RGB\")\n        \n#         text_inputs = self.tokenizer(q_data['question'], padding='max_length', max_length=32, truncation=True, return_tensors=\"pt\")\n#         image_inputs = self.processor(images=image, return_tensors=\"pt\")\n        \n#         ans_str = self.annotations[q_id]['multiple_choice_answer']\n#         label = self.ans_to_id.get(ans_str, 0) \n\n#         return {\n#             'input_ids': text_inputs['input_ids'].squeeze(),\n#             'attention_mask': text_inputs['attention_mask'].squeeze(),\n#             'pixel_values': image_inputs['pixel_values'].squeeze(),\n#             'label': torch.tensor(label)\n#         }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T13:26:54.266201Z","iopub.execute_input":"2025-12-29T13:26:54.266520Z","iopub.status.idle":"2025-12-29T13:26:54.276932Z","shell.execute_reply.started":"2025-12-29T13:26:54.266487Z","shell.execute_reply":"2025-12-29T13:26:54.276220Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class VQAModel(torch.nn.Module):\n#     def __init__(self, num_answers=1000):\n#         super().__init__()\n#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n#         self.clip = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n#         # BERT (768) + CLIP (768) = 1536 features\n#         self.classifier = torch.nn.Sequential(\n#             torch.nn.Linear(1536, 512),\n#             torch.nn.ReLU(),\n#             torch.nn.Linear(512, num_answers)\n#         )\n\n#     def forward(self, input_ids, attention_mask, pixel_values):\n#         # Extract Text Features\n#         text_features = self.bert(input_ids, attention_mask=attention_mask).pooler_output\n#         # Extract Image Features\n#         image_features = self.clip(pixel_values).pooler_output\n#         # Fuse and Classify\n#         fused = torch.cat((text_features, image_features), dim=1)\n#         return self.classifier(fused)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T13:26:59.734983Z","iopub.execute_input":"2025-12-29T13:26:59.735278Z","iopub.status.idle":"2025-12-29T13:26:59.741310Z","shell.execute_reply.started":"2025-12-29T13:26:59.735253Z","shell.execute_reply":"2025-12-29T13:26:59.740524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# from torch.utils.data import DataLoader, random_split\n# from transformers import BertTokenizer, CLIPProcessor\n# from tqdm import tqdm  # This adds a visual progress bar\n# from collections import Counter\n\n\n# # 1. Initialize Components\n# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n\n# # 1. Initialize with 3000 samples\n# # This will look through your JSON and stop once it finds 3000 images on your disk\n# dataset_full = MiniVQADataset(QUEST_PATH, ANNOT_PATH, IMG_DIR, tokenizer, processor)\n# # Inside your class, change 'if len(self.valid_questions) >= 2000' to 3000\n\n# # 2. Define the split (80% for training, 20% for testing)\n# train_count = int(0.8 * len(dataset_full)) # 2400 samples\n# test_count = len(dataset_full) - train_count # 600 samples\n\n# # 3. Perform the split\n# # random_split gives you two subsets of your data\n# train_ds, test_ds = random_split(dataset_full, [train_count, test_count])\n\n\n\n# # Create Dataset and Loader\n# # train_ds = MiniVQADataset(QUEST_PATH, ANNOT_PATH, IMG_DIR, tokenizer, processor)\n# # loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n# # 4. Create separate DataLoaders\n# # The model only \"sees\" the train_loader during training\n# train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n# test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n\n# # 2. Setup Model & Optimizer\n# # model = VQAModel(num_answers=1000).to(device) # num_answers should match your class output\n\n# # 2. Initialize Model with your new 1000-answer limit\n# model = VQAModel(num_answers=1000)\n\n# # 3. ACTIVATE BOTH GPUs\n# if torch.cuda.device_count() > 1:\n#     print(f\"ðŸ”¥ Using {torch.cuda.device_count()} GPUs for training!\")\n#     model = torch.nn.DataParallel(model)\n\n# # 4. Move the model to the device\n# model.to(device)\n\n# # opt = torch.optim.AdamW(model.parameters(), lr=5e-5)\n# opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n# loss_fn = torch.nn.CrossEntropyLoss()\n\n# # 3. Merged Training Loop with Accuracy\n# # NUM_EPOCHS = 1 # Start with 1 to test\n# NUM_EPOCHS = 40 # Start with 1 to test\n# model.train()\n\n# for epoch in range(NUM_EPOCHS):\n#     running_corrects = 0\n#     total_samples = 0\n    \n#     # tqdm creates a progress bar so you don't just see a static number\n#     progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n    \n#     for batch in progress_bar:\n#         # Move batch to GPU\n#         ids = batch['input_ids'].to(device)\n#         mask = batch['attention_mask'].to(device)\n#         pixels = batch['pixel_values'].to(device)\n#         labels = batch['label'].to(device)\n        \n#         # Forward Pass\n#         # opt.zero_grad()\n#         opt.zero_grad(set_to_none=True)\n#         outputs = model(ids, mask, pixels)\n#         loss = loss_fn(outputs, labels)\n        \n#         # --- Accuracy Calculation ---\n#         _, preds = torch.max(outputs, 1)            # Get the predicted answer ID\n#         running_corrects += (preds == labels).sum().item() # Count matches\n#         total_samples += labels.size(0)             # Count total items\n#         current_acc = 100 * running_corrects / total_samples\n#         # ----------------------------\n\n#         # Backward Pass\n#         loss.backward()\n#         opt.step()\n        \n#         # Update progress bar with Loss and Accuracy\n#         progress_bar.set_postfix({\n#             'loss': f'{loss.item():.4f}', \n#             'acc': f'{current_acc:.2f}%'\n#         })\n\n# # 4. Save Final Weights\n# # torch.save(model.state_dict(), \"/kaggle/working/vqa_mini.pth\")\n# # after using 2 GPUs\n# # Use .module to get the original model inside the wrapper\n# torch.save(model.module.state_dict(), \"/kaggle/working/vqa_mini.pth\")\n\n# print(f\"\\nTraining Finished! Final Accuracy: {current_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T13:58:12.721199Z","iopub.execute_input":"2025-12-29T13:58:12.721510Z","iopub.status.idle":"2025-12-29T15:10:59.527631Z","shell.execute_reply.started":"2025-12-29T13:58:12.721482Z","shell.execute_reply":"2025-12-29T15:10:59.526392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Save the weights safely\n# if isinstance(model, torch.nn.DataParallel):\n#     # This reaches inside the multi-GPU wrapper to get the actual weights\n#     torch.save(model.module.state_dict(), \"/kaggle/working/vqa_best_97.pth\")\n#     print(\"Multi-GPU weights saved successfully!\")\n# else:\n#     torch.save(model.state_dict(), \"/kaggle/working/vqa_best_97.pth\")\n#     print(\"Single-GPU weights saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T15:11:12.597910Z","iopub.execute_input":"2025-12-29T15:11:12.598505Z","iopub.status.idle":"2025-12-29T15:11:13.474747Z","shell.execute_reply.started":"2025-12-29T15:11:12.598471Z","shell.execute_reply":"2025-12-29T15:11:13.474090Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing ","metadata":{}},{"cell_type":"code","source":"# model.eval()  # Set model to evaluation mode\n# test_running_corrects = 0\n# test_total_samples = 0\n\n# print(\"Starting Evaluation on Test Set...\")\n\n# # Disable gradient calculations for speed and memory efficiency\n# with torch.no_grad():\n#     # Use tqdm to monitor progress on the 600 test samples\n#     test_progress_bar = tqdm(test_loader, desc=\"Testing\")\n    \n#     for batch in test_progress_bar:\n#         ids = batch['input_ids'].to(device)\n#         mask = batch['attention_mask'].to(device)\n#         pixels = batch['pixel_values'].to(device)\n#         labels = batch['label'].to(device)\n\n#         # Forward Pass\n#         outputs = model(ids, mask, pixels)\n        \n#         # Calculate Accuracy\n#         _, preds = torch.max(outputs, 1)\n#         test_running_corrects += (preds == labels).sum().item()\n#         test_total_samples += labels.size(0)\n        \n#         # Update progress bar\n#         current_test_acc = 100 * test_running_corrects / test_total_samples\n#         test_progress_bar.set_postfix({'test_acc': f'{current_test_acc:.2f}%'})\n\n# final_test_accuracy = 100 * test_running_corrects / test_total_samples\n# print(f\"\\n--- Evaluation Results ---\")\n# print(f\"Final Test Accuracy: {final_test_accuracy:.2f}%\")\n# print(f\"Training Accuracy was: {current_acc:.2f}%\") # This is from your training loop\n# print(f\"The 'Generalization Gap' is: {current_acc - final_test_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T13:18:53.808618Z","iopub.execute_input":"2025-12-29T13:18:53.809227Z","iopub.status.idle":"2025-12-29T13:19:04.130331Z","shell.execute_reply.started":"2025-12-29T13:18:53.809184Z","shell.execute_reply":"2025-12-29T13:19:04.129530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.eval()\n# test_running_corrects = 0\n# test_total_samples = 0\n\n# print(f\"Starting Evaluation on {len(test_ds)} images...\")\n\n# with torch.no_grad():\n#     # We set total=len(test_ds) and update by the batch size manually\n#     test_progress_bar = tqdm(total=len(test_ds), desc=\"Testing Images\")\n    \n#     for batch in test_loader:\n#         ids = batch['input_ids'].to(device)\n#         mask = batch['attention_mask'].to(device)\n#         pixels = batch['pixel_values'].to(device)\n#         labels = batch['label'].to(device)\n\n#         outputs = model(ids, mask, pixels)\n        \n#         _, preds = torch.max(outputs, 1)\n#         batch_corrects = (preds == labels).sum().item()\n#         test_running_corrects += batch_corrects\n#         test_total_samples += labels.size(0)\n        \n#         # Update bar by number of images in this batch (usually 8)\n#         test_progress_bar.update(labels.size(0))\n        \n#         current_test_acc = 100 * test_running_corrects / test_total_samples\n#         test_progress_bar.set_postfix({'test_acc': f'{current_test_acc:.2f}%'})\n\n#     test_progress_bar.close()\n\n# print(f\"\\nFinal Test Accuracy: {current_test_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T16:54:44.039772Z","iopub.execute_input":"2025-12-29T16:54:44.040039Z","iopub.status.idle":"2025-12-29T16:54:44.048861Z","shell.execute_reply.started":"2025-12-29T16:54:44.040017Z","shell.execute_reply":"2025-12-29T16:54:44.047809Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing on Real image","metadata":{}},{"cell_type":"code","source":"# import torch\n# from PIL import Image\n# import torchvision.transforms as transforms\n# from transformers import BertTokenizer\n\n# # --- CONFIGURATION ---\n# # Use the same variable name you used during training!\n# # If your model variable is 'vqa_net', change 'model' to 'vqa_net' below.\n# my_model = model  \n\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased')\n# IMG_SIZE = (224, 224) \n# MAX_LEN = 32           \n\n# # --- 1. DYNAMIC ANSWER MAPPING ---\n# # This uses your existing label_encoder to handle all 1000+ classes\n# def decode_answer(idx):\n#     # 'label_encoder' must be the one you used to fit your training data\n#     return label_encoder.inverse_transform([idx])[0]\n\n# # --- 2. PREPROCESSING ---\n# def preprocess(image_path, question):\n#     img = Image.open(image_path).convert('RGB')\n#     transform = transforms.Compose([\n#         transforms.Resize(IMG_SIZE),\n#         transforms.ToTensor(),\n#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n#     ])\n#     img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n\n#     tokens = TOKENIZER(\n#         question,\n#         padding='max_length',\n#         truncation=True,\n#         max_length=MAX_LEN,\n#         return_tensors=\"pt\"\n#     ).to(DEVICE)\n    \n#     return img_tensor, tokens['input_ids'], tokens['attention_mask']\n\n# # --- 3. RUN PREDICTION ---\n# def get_answer(img_path, q_text):\n#     img_t, input_ids, att_mask = preprocess(img_path, q_text)\n    \n#     my_model.eval() \n#     with torch.no_grad():\n#         # Ensure the arguments (img, ids, mask) match your model's forward() order\n#         output = my_model(img_t, input_ids, att_mask) \n#         idx = torch.argmax(output, dim=1).item()\n        \n#     prediction = decode_answer(idx)\n#     print(f\"Question: {q_text}\")\n#     print(f\"Predicted Answer: {prediction}\")\n\n# # --- TEST ---\n# get_answer(\"/kaggle/input/doggyy/dog.jpg\", \"tell me the animal shown in image\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-29T16:52:29.585316Z","iopub.execute_input":"2025-12-29T16:52:29.586150Z","iopub.status.idle":"2025-12-29T16:52:40.157651Z","shell.execute_reply.started":"2025-12-29T16:52:29.586109Z","shell.execute_reply":"2025-12-29T16:52:40.156551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport torch\nimport time\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\nfrom torch import nn\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torchvision import transforms\nfrom collections import Counter\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\n\nimport os\n# set cuda\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T19:28:58.434339Z","iopub.execute_input":"2026-01-02T19:28:58.434924Z","iopub.status.idle":"2026-01-02T19:28:58.440151Z","shell.execute_reply.started":"2026-01-02T19:28:58.434893Z","shell.execute_reply":"2026-01-02T19:28:58.439488Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Paths of uploaded dataset files\nIMG_DIR = \"/kaggle/input/mini-coco2014-dataset-for-image-captioning/Images\"\nQUEST_PATH = \"/kaggle/input/vqatext/v2_Questions_Train_mscoco/v2_OpenEnded_mscoco_train2014_questions.json\"\nANNOT_PATH = \"/kaggle/input/vqatext/v2_Annotations_Train_mscoco/v2_mscoco_train2014_annotations.json\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T19:29:15.992279Z","iopub.execute_input":"2026-01-02T19:29:15.992873Z","iopub.status.idle":"2026-01-02T19:29:15.996088Z","shell.execute_reply.started":"2026-01-02T19:29:15.992843Z","shell.execute_reply":"2026-01-02T19:29:15.995498Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Image pre-processing : to properly utilize ResNET50 (ImageNet)\nval_transform = transforms.Compose([\n    transforms.Resize(256),          # Resize shorter side to 256\n    transforms.CenterCrop(224),      # crop center to 224X224\n    transforms.ToTensor(),           # convert to Tensor from PIL object (Python Image loader)\n    transforms.Normalize(\n        mean = [0.485, 0.456, 0.406],\n        std  = [0.229, 0.224, 0.225]\n    )\n])\n\n# Training transform - Data Augmentation \ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),    # Random crop + resize to 224X224\n    transforms.RandomHorizontalFlip(),    # randomly flip horizontally\n    transforms.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.3),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean = [0.485, 0.456, 0.406],\n        std  = [0.229, 0.224, 0.225]\n    )\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T19:29:31.485706Z","iopub.execute_input":"2026-01-02T19:29:31.486600Z","iopub.status.idle":"2026-01-02T19:29:31.491788Z","shell.execute_reply.started":"2026-01-02T19:29:31.486567Z","shell.execute_reply":"2026-01-02T19:29:31.491158Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class ImageEncoder(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)   # Loading a RestNet50 pretrained on ImageNET (IMAGENET1K_V2 is a strong, modern weight set)\n\n    # Remove last classification layer[2048->1000] and replace it with [2048->512]\n    self.resnet.fc = nn.Linear(2048, 512)\n\n    # Freeze all resnet parameters except except last fc (fully-connected) layer\n    for name, param in self.resnet.named_parameters():\n      if not name.startswith(\"fc\"):\n        param.requires_grad = False\n\n  # Forward pass\n  def forward(self, x):\n    return self.resnet(x)    # [B, 512]\n  # Input: x is batch of images shaped [B, 3, 224, 224], preprocessed with transforms (resize, crop, normalize)\n  # Output: a batch of embeddings [B, 512] from the modified ResNet50\n  # Reason:  this vector is visual reprstn you'll feed into VQA model, often concatenated, attended over, or fused with text embeddings           \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T19:29:55.238608Z","iopub.execute_input":"2026-01-02T19:29:55.239164Z","iopub.status.idle":"2026-01-02T19:29:55.244161Z","shell.execute_reply.started":"2026-01-02T19:29:55.239134Z","shell.execute_reply":"2026-01-02T19:29:55.243325Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class TextEncoder(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.bert = BertModel.from_pretrained(\"bert-base-uncased\")  # all text is lowercased \n    # Projection layer\n    self.fc = nn.Linear(768, 512)  # Adds a fc(linear) layer to project BERT's output dim.(768) down to 512\n\n    # Freeze all BERT parameters > they won't be updated during training\n    for param in self.bert.parameter():\n      param.requires_grad = False\n    # - Only the projection layer (fc) learns to adapt BERTâ€™s embeddings to your VQA task.\n\n    # Forward pass\n    def forward(self, input_ids, attention_mask):\n      output = self.bert(\n          input_ids = input_ids,\n          attention_mask = attention_mask\n      )    # output: contains hidden states for each token & pooled outputs\n\n      # CLS token extraction \n      cls_embedding = output.last_hidden_state[:, 0, :]  # [B, 768]-> one vector per q.\n      cls_embedding = self.fc(cls_embedding)   # [B, 512]   projects the CLS embeddings down to 512 dimensions \n      \n      return cls_embedding   \n      # - This vector represents the semantic meaning of the question, ready to be combined with the image embedding.\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T19:30:16.984398Z","iopub.execute_input":"2026-01-02T19:30:16.985131Z","iopub.status.idle":"2026-01-02T19:30:16.990462Z","shell.execute_reply.started":"2026-01-02T19:30:16.985099Z","shell.execute_reply":"2026-01-02T19:30:16.989738Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class VQADataset(Dataset):\n  def __init__(self, image_dir, questions_path, annotations_path, tokenizer, answer_to_idx, transform):\n\n    self.image_dir = image_dir\n    self.transform = transform\n    self.tokenizer = tokenizer\n    self.answer_to_idx = answer_to_idx\n\n    with open(questions_path, 'r') as f:\n      self.questions = json.load(f)[\"questions\"]\n    with open(annotations_path, 'r') as f:\n      anns = json.load(f)[\"annotations\"]\n\n    self.ann_map = {ann[\"questions_id\"]: ann for ann in anns}  # building a dictionary mapping q_id -> annotations, making it easy to lookup for answer for a given q.\n  \n  def __len__(self):\n    return len(self.questions)    # needed by Pytorch's dataloader\n\n  def __getitem__(self, idx):\n    q = self.questions[idx]\n    qid = q[\"questions_id\"]\n\n    # Image loading\n    img_name = f\"COCO_train2014_{q['image_id']:012d}.jpg\"   # :012d = zero-pads the ID to 12 digits (COCO naming convention)\n    img_path = os.path.join(self.image_dir, img_name)\n    image = Image.open(img_path).convert(\"RGB\")\n    image = self.transform(image)\n    # Constructing the full path, opening image using PIL -> converts to RGB, then applying pre-processing transforms\n\n\n    # Question Tokenization \n    tokens = self.tokenizer(\n        q[\"question\"], \n        padding = \"max_length\",\n        truncation = True,\n        max_length = 32,\n        return_tensors = \"pt\"\n    ) \n    # tokenizes the qs. using BERT tokenizer, pads/truncates to fixed lengths (32 tokens), returns PyTorch tensors (input_ids, attention_mask)\n\n    # Answer label \n    answer = self.ann_map[qid]['multiple_choice_answer']\n    label = self.answer_to_idx.get(answer, -1)      \n\n    \n    # Return a tuple \n    return (\n        image, \n        tokens['input_ids'].squueze(0),\n        tokens['attention_mask'].squueze(0),\n        torch.tensor(label)          \n    )\n    #   This tuple is exactly what the model needs: \n    # visual features + text features + answer label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T19:30:43.837919Z","iopub.execute_input":"2026-01-02T19:30:43.838567Z","iopub.status.idle":"2026-01-02T19:30:43.845598Z","shell.execute_reply.started":"2026-01-02T19:30:43.838534Z","shell.execute_reply":"2026-01-02T19:30:43.844846Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class VQAModel(nn.Module):\n  def __init__(self, num_answers):\n    super().__init__()\n\n    # Image Encoder \n    self.cnn = resnet50(pretrained=True)\n    # Backbone: loads resnet50 pretrained on ImageNet for strong visual features\n    self.cnn.fc = nn.Identity()\n    # Head removal - replace the final classification layer with Identity - so the output is 2048-dim pooled feature instead of 1000 class-logits\n\n    for p in self.cnn.parameters():\n      p.requires_grad = False\n    # Freezing: stopping all CNN parameters from updating - keeping pre-trained features fixed\n\n\n    #   Text encoder (BERT) -\n    self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n    # Loads BERT-base (uncased) for contextual q. embeddings \n    for layer in self.bert.encoder.layer[:8]:\n      for p in layer.parameters():\n        p.requires_grad = False\n    # Partial freezing : Freezes the first 8 transformer layers; keeps the top 4 trainable,  - lower layer capture general semantics, upper layer adapts to task \n\n\n    #  FUSION and CLASSIFIER HEAD \n    self.classifier = nn.Sequential(\n        nn.Linear(2048 + 768, 512),\n        nn.ReLU(),\n        nn.Dropout(0.5), \n        nn.Linear(512, num_answers)\n    )\n    # Projection: Linear(2816->512) compresses fused features\n    # Activation: ReLU  to add non--linearity\n    # Regularization: Dropout(0.5) to combat over-fitting\n    # Output: Linear(512->num_answers) produces logit over answer vocabulary\n\n\n  #  FORWARD  PASS\n  def forward(self, image, input_ids, attention_mask):\n    img_feat = self.cnn(image)\n    #  Image features: Passes preprocessed images [B,3,224,224] through ResNet50 â†’ global features [B,2048]\n    \n    txt_feat = self.bert(\n        input_ids = input_ids,\n        attention_mask = attention_mask\n    ).last_hidden_state[:, 0, :]    # running BERT on tokenized questions\n\n    fused = torch.cat([img_feat, txt_feat], dim=1)\n    # fusion : concat. img & text features along feature dimension-> [B, 2816]\n    return self.classifier(fused)  \n    #  Prediction: Applies the classifier to produce logits [B, num_answers]\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T19:31:21.175442Z","iopub.execute_input":"2026-01-02T19:31:21.176172Z","iopub.status.idle":"2026-01-02T19:31:21.182750Z","shell.execute_reply.started":"2026-01-02T19:31:21.176140Z","shell.execute_reply":"2026-01-02T19:31:21.182197Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train_one_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n\n    total_loss = 0.0\n    correct = 0\n    total = 0\n\n    for batch in loader:\n        images = batch[\"image\"].to(device)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"label\"].to(device)\n\n        mask = labels != -1\n        if mask.sum() == 0:\n            continue\n\n        outputs = model(images, input_ids, attention_mask)\n\n        loss = criterion(outputs[mask], labels[mask])\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        # ðŸ”¹ Training accuracy calculation\n        preds = outputs.argmax(dim=1)\n        correct += (preds[mask] == labels[mask]).sum().item()\n        total += mask.sum().item()\n\n    train_loss = total_loss / len(loader)\n    train_acc = correct / total if total > 0 else 0\n\n    return train_loss, train_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T19:32:22.850947Z","iopub.execute_input":"2026-01-02T19:32:22.851746Z","iopub.status.idle":"2026-01-02T19:32:22.857644Z","shell.execute_reply.started":"2026-01-02T19:32:22.851713Z","shell.execute_reply":"2026-01-02T19:32:22.856930Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# needed bcz. the model needs a fixed set of possible answers (classes) to predict form\nfrom collections import Counter\nimport json\n\nTOP_K = 1000   # number of most frequent answers \n# Counter: a dictionary subclass that counts occurences of item\n# TOP_K: defines how many of the most frequent answers you'll keep in your vocabulary (default=1000)\n\ndef build_answer_vocab(annotation_file, TOP_K):\n  with open(annotation_file, 'r') as f:\n    data = json.load(f)\n\n  #  Collect answers \n  answers = []\n  for ann in data['annotations']:\n    # COCO VQA has 10 answers per question\n    for ans in ann[\"answers\"]:\n      # normalize answer text \n      answer = ans['answer'].lower().strip()\n      answers.append(answer)\n\n  counter = Counter(answers)\n\n  # take top-k most frequent answers\n  most_common_answers = counter.most_common(TOP_K)\n\n  answer_to_idx = {}\n  idx_to_answer = {}\n\n  for idx, (answer, _) in enumerate(most_common_answers):\n    answer_to_idx[answer] = idx\n    idx_to_answer[idx] = answer\n\n  print(f\"Total unique answers: {len(counter)}\")\n  print(f\"Using top-{TOP_K} answers\")\n\n  return answer_to_idx, idx_to_answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T19:33:10.846752Z","iopub.execute_input":"2026-01-02T19:33:10.847216Z","iopub.status.idle":"2026-01-02T19:33:10.853136Z","shell.execute_reply.started":"2026-01-02T19:33:10.847184Z","shell.execute_reply":"2026-01-02T19:33:10.852357Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnum_answers = 1500\nmodel = VQAModel(num_answers).to(device)\n\n# Loss fn / calculating criteria\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer - ADAM (used here)\noptimizer = torch.optim.Adam(\n    filter (lambda p: p.requires_grad, model.parameters()),\n    lr = 2e-4\n)\n\nscheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size = 3,\n    gamma = 0.3\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T19:33:39.653523Z","iopub.execute_input":"2026-01-02T19:33:39.654157Z","iopub.status.idle":"2026-01-02T19:33:40.474085Z","shell.execute_reply.started":"2026-01-02T19:33:39.654129Z","shell.execute_reply":"2026-01-02T19:33:40.473302Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NEW FINAL MERGED GEM___ code","metadata":{}},{"cell_type":"code","source":"import json\nimport torch\nimport time\nimport os\nimport numpy as np\nfrom PIL import Image\nfrom collections import Counter\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm.auto import tqdm\nimport sys\n\n# 1. Setup Device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\n\n# 2. Define Transforms\n# Image pre-processing : to properly utilize ResNET50 (ImageNet)\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),   # Resize directly to 224x224 for simplicity\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:13:40.036657Z","iopub.execute_input":"2026-01-03T04:13:40.037313Z","iopub.status.idle":"2026-01-03T04:13:40.044464Z","shell.execute_reply.started":"2026-01-03T04:13:40.037283Z","shell.execute_reply":"2026-01-03T04:13:40.043915Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# 3. Helper: Build Answer Vocabulary\ndef build_answer_vocab(annotations, top_k=1000):\n    all_answers = []\n    for ann in annotations:\n        for ans in ann[\"answers\"]:\n            all_answers.append(ans['answer'].lower().strip())\n    \n    counter = Counter(all_answers)\n    most_common = counter.most_common(top_k)\n    \n    answer_to_idx = {ans: i for i, (ans, _) in enumerate(most_common)}\n    return answer_to_idx\n\n# 4. Modified Dataset Class (Accepts data lists instead of paths)\nclass VQADataset(Dataset):\n    def __init__(self, image_dir, questions_list, ann_map, tokenizer, answer_to_idx, transform):\n        self.image_dir = image_dir\n        self.questions = questions_list\n        self.ann_map = ann_map\n        self.tokenizer = tokenizer\n        self.answer_to_idx = answer_to_idx\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, idx):\n        q = self.questions[idx]\n        qid = q[\"question_id\"]\n        \n        # Image Loading\n        img_name = f\"COCO_train2014_{q['image_id']:012d}.jpg\"\n        img_path = os.path.join(self.image_dir, img_name)\n        \n        # Safety check if image exists (optional but good for debugging)\n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n        except:\n            # Create a dummy black image if file missing (prevents crash)\n            image = Image.new('RGB', (224, 224))\n            \n        image = self.transform(image)\n\n        # Tokenization\n        tokens = self.tokenizer(\n            q[\"question\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=32,\n            return_tensors=\"pt\"\n        )\n\n        # Label\n        ann = self.ann_map.get(qid)\n        label = -1\n        if ann:\n            # We take the 'multiple_choice_answer'\n            ans_str = ann['multiple_choice_answer'].lower().strip()\n            label = self.answer_to_idx.get(ans_str, -1)\n\n        # Fixed typo: squueze -> squeeze\n        return {\n            \"image\": image,\n            \"input_ids\": tokens['input_ids'].squeeze(0),\n            \"attention_mask\": tokens['attention_mask'].squeeze(0),\n            \"label\": torch.tensor(label, dtype=torch.long)\n        }\n\n# 5. VQA Model (ResNet + BERT)\n# class VQAModel(nn.Module):\n#     def __init__(self, num_answers):\n#         super().__init__()\n        \n#         # Image Encoder (ResNet50)\n#         self.cnn = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n#         self.cnn.fc = nn.Identity() # Removes the last classification layer\n#         for p in self.cnn.parameters():\n#             p.requires_grad = False # Freeze CNN\n            \n#         # Text Encoder (BERT)\n#         self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n#         for layer in self.bert.encoder.layer[:8]: # Freeze first 8 layers\n#             for p in layer.parameters():\n#                 p.requires_grad = False\n                \n#         # Classifier\n#         self.classifier = nn.Sequential(\n#             nn.Linear(2048 + 768, 512),\n#             nn.ReLU(),\n#             nn.Dropout(0.5),\n#             nn.Linear(512, num_answers)\n#         )\n\n#     def forward(self, image, input_ids, attention_mask):\n#         img_feat = self.cnn(image)             # [B, 2048]\n        \n#         bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n#         txt_feat = bert_out.last_hidden_state[:, 0, :] # CLS token [B, 768]\n        \n#         fused = torch.cat([img_feat, txt_feat], dim=1) # [B, 2816]\n#         logits = self.classifier(fused)\n#         return logits\n\nclass VQAModel(nn.Module):\n    def __init__(self, num_answers):\n        super().__init__()\n        \n        # 1. Visual Encoder (ResNet50)\n        self.cnn = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n        self.cnn.fc = nn.Identity() \n        \n        # UNFREEZE TRICK: Train the last block (layer4) to adapt to VQA\n        for name, param in self.cnn.named_parameters():\n            if \"layer4\" in name:\n                param.requires_grad = True \n            else:\n                param.requires_grad = False\n            \n        # 2. Text Encoder (BERT)\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n        # Freeze first 8 layers, train last 4\n        for layer in self.bert.encoder.layer[:8]: \n            for p in layer.parameters():\n                p.requires_grad = False\n                \n        # 3. FUSION LAYERS (The Fix)\n        # Project both to 1024 dim\n        self.img_proj = nn.Linear(2048, 1024)\n        self.txt_proj = nn.Linear(768, 1024)\n        \n        self.dropout = nn.Dropout(0.5)\n        \n        # 4. Classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_answers)\n        )\n\n    def forward(self, image, input_ids, attention_mask):\n        # Image Features\n        img_feat = self.cnn(image)         # [B, 2048]\n        img_feat = self.img_proj(img_feat) # [B, 1024]\n        \n        # Text Features\n        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        txt_feat = bert_out.last_hidden_state[:, 0, :] # [B, 768]\n        txt_feat = self.txt_proj(txt_feat)             # [B, 1024]\n        \n        # KEY CHANGE: Element-wise Multiplication\n        fused = img_feat * txt_feat  \n        \n        fused = self.dropout(fused)\n        logits = self.classifier(fused)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:14:15.471540Z","iopub.execute_input":"2026-01-03T04:14:15.472315Z","iopub.status.idle":"2026-01-03T04:14:15.485351Z","shell.execute_reply.started":"2026-01-03T04:14:15.472282Z","shell.execute_reply":"2026-01-03T04:14:15.484549Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# def train_one_epoch(model, loader, optimizer, criterion, device):\n#     model.train()\n#     total_loss = 0.0\n#     correct = 0\n#     total = 0\n\n#     # TQDM wraps the loader to show a progress bar\n#     # leave=False keeps the output clean (clears bar after epoch)\n#     loop = tqdm(loader, desc=\"Training\", leave=False)\n\n#     for batch in loop:\n#         images = batch[\"image\"].to(device)\n#         input_ids = batch[\"input_ids\"].to(device)\n#         attention_mask = batch[\"attention_mask\"].to(device)\n#         labels = batch[\"label\"].to(device)\n\n#         mask = labels != -1\n#         if mask.sum() == 0:\n#             continue\n\n#         optimizer.zero_grad()\n#         outputs = model(images, input_ids, attention_mask)\n        \n#         loss = criterion(outputs[mask], labels[mask])\n        \n#         loss.backward()\n#         optimizer.step()\n\n#         total_loss += loss.item()\n        \n#         # Accuracy calculation\n#         preds = outputs.argmax(dim=1)\n#         correct += (preds[mask] == labels[mask]).sum().item()\n#         total += mask.sum().item()\n\n#         # Update the progress bar with current loss\n#         loop.set_postfix(loss=loss.item())\n\n#     avg_loss = total_loss / len(loader)\n#     avg_acc = correct / total if total > 0 else 0\n#     return avg_loss, avg_acc\n\ndef train_one_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n\n    loop = tqdm(loader, desc=\"Training\", leave=False)\n    \n    for batch in loop:\n        images = batch[\"image\"].to(device)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"label\"].to(device)\n\n        mask = labels != -1\n        if mask.sum() == 0: continue\n\n        optimizer.zero_grad()\n        outputs = model(images, input_ids, attention_mask)\n        \n        loss = criterion(outputs[mask], labels[mask])\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        \n        # Calculate running stats\n        preds = outputs.argmax(dim=1)\n        correct += (preds[mask] == labels[mask]).sum().item()\n        total += mask.sum().item()\n        \n        # --- LIVE ACCURACY UPDATE ---\n        # Calculate current average accuracy so far\n        current_acc = correct / total if total > 0 else 0\n        \n        # Update the progress bar\n        loop.set_postfix(loss=loss.item(), acc=f\"{current_acc:.4f}\")\n        \n    train_acc = correct / total if total > 0 else 0\n    train_loss = total_loss / len(loader)\n    \n    return train_loss, train_acc\n\n\ndef validate(model, loader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    \n    # Add TQDM for validation as well\n    loop = tqdm(loader, desc=\"Validating\", leave=False)\n    \n    with torch.no_grad():\n        for batch in loop:\n            images = batch[\"image\"].to(device)\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"label\"].to(device)\n\n            mask = labels != -1\n            if mask.sum() == 0:\n                continue\n\n            outputs = model(images, input_ids, attention_mask)\n            preds = outputs.argmax(dim=1)\n            \n            correct += (preds[mask] == labels[mask]).sum().item()\n            total += mask.sum().item()\n            \n    return correct / total if total > 0 else 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:14:30.955567Z","iopub.execute_input":"2026-01-03T04:14:30.956282Z","iopub.status.idle":"2026-01-03T04:14:30.965895Z","shell.execute_reply.started":"2026-01-03T04:14:30.956251Z","shell.execute_reply":"2026-01-03T04:14:30.965224Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# --- CONFIGURATION ---\nIMG_DIR = \"/kaggle/input/mini-coco2014-dataset-for-image-captioning/Images\"\nQUEST_PATH = \"/kaggle/input/vqatext/v2_Questions_Train_mscoco/v2_OpenEnded_mscoco_train2014_questions.json\"\nANNOT_PATH = \"/kaggle/input/vqatext/v2_Annotations_Train_mscoco/v2_mscoco_train2014_annotations.json\"\nTOP_K = 1000\nBATCH_SIZE = 32\n\n# 1. Load Data\nprint(\"Loading JSON data...\")\nwith open(QUEST_PATH, 'r') as f:\n    questions_json = json.load(f)\n    all_questions = questions_json[\"questions\"]\n\nwith open(ANNOT_PATH, 'r') as f:\n    annotations_json = json.load(f)\n    all_annotations = annotations_json[\"annotations\"]\n\n# Map question_id to annotation\nann_map = {ann[\"question_id\"]: ann for ann in all_annotations}\n\n# 2. Build Vocabulary\nprint(\"Building Vocabulary...\")\nanswer_to_idx = build_answer_vocab(all_annotations, TOP_K)\nnum_classes = len(answer_to_idx)\nprint(f\"Vocab size: {num_classes}\")\n\n# 3. Train/Test Split\n# We split the list of questions, not the file paths\nprint(\"Splitting data...\")\ntrain_questions, val_questions = train_test_split(\n    all_questions, \n    test_size=0.2, \n    random_state=42\n)\nprint(f\"Train size: {len(train_questions)}, Val size: {len(val_questions)}\")\n\n# 4. Create Datasets & Loaders\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\ntrain_dataset = VQADataset(IMG_DIR, train_questions, ann_map, tokenizer, answer_to_idx, train_transform)\nval_dataset = VQADataset(IMG_DIR, val_questions, ann_map, tokenizer, answer_to_idx, val_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\n# 5. Initialize Model\n# model = VQAModel(num_answers=num_classes).to(device)\n\nmodel = VQAModel(num_answers=num_classes) # Don't send to .to(device) yet\n\n# Check if multiple GPUs are available\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    # This wraps the model to split batches across both GPUs\n    model = nn.DataParallel(model)\n\n# Now send the model to the primary device (cuda:0)\nmodel = model.to(device)\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n\n# 6. Run Training\nnum_epochs = 6 # Reduced for testing, increase later\nbest_val_acc = 0.0\n\nprint(\"Starting training...\")\n# for epoch in range(num_epochs):\n#     start_time = time.time()\n    \n#     train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n#     val_acc = validate(model, val_loader, device)\n    \n#     elapsed = time.time() - start_time\n    \n#     print(f\"Epoch {epoch+1}/{num_epochs} | Time: {elapsed:.0f}s\")\n#     print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n\n#     torch.save(model.state_dict(), \"vqa_last.pth\")\n    \n#     if val_acc > best_val_acc:\n#         best_val_acc = val_acc\n#         torch.save(model.state_dict(), \"vqa_best.pth\")\n#         print(\"Saved best model.\")\n\nprint(\"Starting training...\")\n\nfor epoch in range(num_epochs):\n    \n    # Train\n    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n    \n    # Validate\n    val_acc = validate(model, val_loader, device)\n    \n    # Clean print after the progress bars are done\n    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n\n    # torch.save(model.state_dict(), \"vqa_last.pth\")\n    torch.save(model.module.state_dict(), \"vqa_last.pth\")\n    print(\"saved last model\")\n    \n    # Save checkpoints\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        # torch.save(model.state_dict(), \"vqa_best.pth\")\n        torch.save(model.module.state_dict(), \"vqa_best.pth\")\n        print(\" -> Saved best model.\")\n    \n    !zip model.zip vqa_best.pth vqa_last.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T04:14:36.235555Z","iopub.execute_input":"2026-01-03T04:14:36.236128Z"}},"outputs":[{"name":"stdout","text":"Loading JSON data...\nBuilding Vocabulary...\nVocab size: 1000\nSplitting data...\nTrain size: 355005, Val size: 88752\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4d8fc32579a439d9f2d6a0cdb259c09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64f9b4f5983644c59a3651672244eab8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffe13b35f2b548668056d162ffb38edd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f713677dc7b446cbff6a8c05d23a2f6"}},"metadata":{}},{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 185MB/s] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ad5249423c9456ea0663d67201efd66"}},"metadata":{}},{"name":"stdout","text":"Using 2 GPUs!\nStarting training...\nStarting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/11094 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/2774 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1/6 | Train Loss: 2.5254 | Train Acc: 0.3180 | Val Acc: 0.3629\nsaved last model\n -> Saved best model.\n  adding: vqa_best.pth (deflated 7%)\n  adding: vqa_last.pth (deflated 7%)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/11094 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/2774 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2/6 | Train Loss: 2.0148 | Train Acc: 0.3642 | Val Acc: 0.3790\nsaved last model\n -> Saved best model.\nupdating: vqa_best.pth (deflated 7%)\nupdating: vqa_last.pth (deflated 7%)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/11094 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52e40f99a2eb451c8995b36568717dfa"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/working\"))\nprint(os.listdir(\"/kaggle/working/.virtual_documents\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip model.zip vqa_best.pth vqa_last.pth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T03:37:28.907548Z","iopub.execute_input":"2026-01-03T03:37:28.907972Z","iopub.status.idle":"2026-01-03T03:37:29.033884Z","shell.execute_reply.started":"2026-01-03T03:37:28.907942Z","shell.execute_reply":"2026-01-03T03:37:29.032431Z"}},"outputs":[{"name":"stdout","text":"\tzip warning: name not matched: vqa_best.pth\n\tzip warning: name not matched: vqa_last.pth\n\nzip error: Nothing to do! (model.zip)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import json\nfrom IPython.display import FileLink\n\n# 1. Save the file locally in the kernel\nwith open('vocab.json', 'w') as f:\n    json.dump(answer_to_idx, f)\n\n# 2. Generate a download link\nFileLink('vocab.json')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
