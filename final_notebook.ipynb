{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T11:05:24.596160Z",
     "iopub.status.busy": "2026-01-03T11:05:24.595623Z",
     "iopub.status.idle": "2026-01-03T11:05:24.603450Z",
     "shell.execute_reply": "2026-01-03T11:05:24.602662Z",
     "shell.execute_reply.started": "2026-01-03T11:05:24.596126Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "\n",
    "# 1. Setup Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-preprocessing + Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define Transforms\n",
    "# Image pre-processing : to properly utilize ResNET50 (ImageNet)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),   # Resize directly to 224x224 for simplicity\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Training transform - Data Augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.3, saturation=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, image_dir, questions_list, ann_map, tokenizer, answer_to_idx, transform):\n",
    "        self.image_dir = image_dir\n",
    "        self.questions = questions_list\n",
    "        self.ann_map = ann_map\n",
    "        self.tokenizer = tokenizer\n",
    "        self.answer_to_idx = answer_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q = self.questions[idx]\n",
    "        qid = q[\"question_id\"]\n",
    "        \n",
    "        # Image Loading\n",
    "        img_name = f\"COCO_train2014_{q['image_id']:012d}.jpg\"\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Safety check if image exists (optional but good for debugging)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            # Create a dummy black image if file missing (prevents crash)\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "            \n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Tokenization\n",
    "        tokens = self.tokenizer(\n",
    "            q[\"question\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=32,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Label\n",
    "        ann = self.ann_map.get(qid)\n",
    "        label = -1\n",
    "        if ann:\n",
    "            # We take the 'multiple_choice_answer'\n",
    "            ans_str = ann['multiple_choice_answer'].lower().strip()\n",
    "            label = self.answer_to_idx.get(ans_str, -1)\n",
    "\n",
    "        # Fixed typo: squueze -> squeeze\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"input_ids\": tokens['input_ids'].squeeze(0),\n",
    "            \"attention_mask\": tokens['attention_mask'].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA Model (ResNet + BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. VQA Model (ResNet + BERT)\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, num_answers):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Visual Encoder (ResNet50)\n",
    "        self.cnn = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.cnn.fc = nn.Identity() \n",
    "        \n",
    "        # UNFREEZE TRICK: Train the last block (layer4) to adapt to VQA\n",
    "        for name, param in self.cnn.named_parameters():\n",
    "            if \"layer4\" in name:\n",
    "                param.requires_grad = True \n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "            \n",
    "        # 2. Text Encoder (BERT)\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        # Freeze first 8 layers, train last 4\n",
    "        for layer in self.bert.encoder.layer[:8]: \n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "                \n",
    "        # 3. FUSION LAYERS (The Fix)\n",
    "        # Project both to 1024 dim\n",
    "        self.img_proj = nn.Linear(2048, 1024)\n",
    "        self.txt_proj = nn.Linear(768, 1024)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # 4. Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_answers)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Image Features\n",
    "        img_feat = self.cnn(image)         # [B, 2048]\n",
    "        img_feat = self.img_proj(img_feat) # [B, 1024]\n",
    "        \n",
    "        # Text Features\n",
    "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        txt_feat = bert_out.last_hidden_state[:, 0, :] # [B, 768]\n",
    "        txt_feat = self.txt_proj(txt_feat)             # [B, 1024]\n",
    "        \n",
    "        # KEY CHANGE: Element-wise Multiplication\n",
    "        fused = img_feat * txt_feat  \n",
    "        \n",
    "        fused = self.dropout(fused)\n",
    "        logits = self.classifier(fused)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in loop:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        mask = labels != -1\n",
    "        if mask.sum() == 0: continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, input_ids, attention_mask)\n",
    "        \n",
    "        loss = criterion(outputs[mask], labels[mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate running stats\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds[mask] == labels[mask]).sum().item()\n",
    "        total += mask.sum().item()\n",
    "        \n",
    "        # --- LIVE ACCURACY UPDATE ---\n",
    "        # Calculate current average accuracy so far\n",
    "        current_acc = correct / total if total > 0 else 0\n",
    "        \n",
    "        # Update the progress bar\n",
    "        loop.set_postfix(loss=loss.item(), acc=f\"{current_acc:.4f}\")\n",
    "        \n",
    "    train_acc = correct / total if total > 0 else 0\n",
    "    train_loss = total_loss / len(loader)\n",
    "    \n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "# Validation fn\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Add TQDM for validation as well\n",
    "    loop = tqdm(loader, desc=\"Validating\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loop:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            mask = labels != -1\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            \n",
    "            correct += (preds[mask] == labels[mask]).sum().item()\n",
    "            total += mask.sum().item()\n",
    "            \n",
    "    return correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T11:05:24.604784Z",
     "iopub.status.busy": "2026-01-03T11:05:24.604482Z",
     "iopub.status.idle": "2026-01-03T11:05:24.634873Z",
     "shell.execute_reply": "2026-01-03T11:05:24.634159Z",
     "shell.execute_reply.started": "2026-01-03T11:05:24.604759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  Helper: Building Answer Vocabulary\n",
    "def build_answer_vocab(annotations, top_k=1000):\n",
    "    all_answers = []\n",
    "    for ann in annotations:\n",
    "        for ans in ann[\"answers\"]:\n",
    "            all_answers.append(ans['answer'].lower().strip())\n",
    "    \n",
    "    counter = Counter(all_answers)\n",
    "    most_common = counter.most_common(top_k)\n",
    "    \n",
    "    answer_to_idx = {ans: i for i, (ans, _) in enumerate(most_common)}\n",
    "    return answer_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Execution / Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-01-03T11:02:40.916Z",
     "iopub.execute_input": "2026-01-03T04:14:36.236128Z",
     "iopub.status.busy": "2026-01-03T04:14:36.235555Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON data...\n",
      "Building Vocabulary...\n",
      "Vocab size: 1000\n",
      "Splitting data...\n",
      "Train size: 355005, Val size: 88752\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d8fc32579a439d9f2d6a0cdb259c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f9b4f5983644c59a3651672244eab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe13b35f2b548668056d162ffb38edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f713677dc7b446cbff6a8c05d23a2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 185MB/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad5249423c9456ea0663d67201efd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "Starting training...\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2774 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6 | Train Loss: 2.5254 | Train Acc: 0.3180 | Val Acc: 0.3629\n",
      "saved last model\n",
      " -> Saved best model.\n",
      "  adding: vqa_best.pth (deflated 7%)\n",
      "  adding: vqa_last.pth (deflated 7%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2774 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/6 | Train Loss: 2.0148 | Train Acc: 0.3642 | Val Acc: 0.3790\n",
      "saved last model\n",
      " -> Saved best model.\n",
      "updating: vqa_best.pth (deflated 7%)\n",
      "updating: vqa_last.pth (deflated 7%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e40f99a2eb451c8995b36568717dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/11094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "IMG_DIR = \"/kaggle/input/mini-coco2014-dataset-for-image-captioning/Images\"\n",
    "QUEST_PATH = \"/kaggle/input/vqatext/v2_Questions_Train_mscoco/v2_OpenEnded_mscoco_train2014_questions.json\"\n",
    "ANNOT_PATH = \"/kaggle/input/vqatext/v2_Annotations_Train_mscoco/v2_mscoco_train2014_annotations.json\"\n",
    "TOP_K = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 1. Load Data\n",
    "print(\"Loading JSON data...\")\n",
    "with open(QUEST_PATH, 'r') as f:\n",
    "    questions_json = json.load(f)\n",
    "    all_questions = questions_json[\"questions\"]\n",
    "\n",
    "with open(ANNOT_PATH, 'r') as f:\n",
    "    annotations_json = json.load(f)\n",
    "    all_annotations = annotations_json[\"annotations\"]\n",
    "\n",
    "# Map question_id to annotation\n",
    "ann_map = {ann[\"question_id\"]: ann for ann in all_annotations}\n",
    "\n",
    "# 2. Build Vocabulary\n",
    "print(\"Building Vocabulary...\")\n",
    "answer_to_idx = build_answer_vocab(all_annotations, TOP_K)\n",
    "num_classes = len(answer_to_idx)\n",
    "print(f\"Vocab size: {num_classes}\")\n",
    "\n",
    "# 3. Train/Test Split\n",
    "# We split the list of questions, not the file paths\n",
    "print(\"Splitting data...\")\n",
    "train_questions, val_questions = train_test_split(\n",
    "    all_questions, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "print(f\"Train size: {len(train_questions)}, Val size: {len(val_questions)}\")\n",
    "\n",
    "# 4. Create Datasets & Loaders\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_dataset = VQADataset(IMG_DIR, train_questions, ann_map, tokenizer, answer_to_idx, train_transform)\n",
    "val_dataset = VQADataset(IMG_DIR, val_questions, ann_map, tokenizer, answer_to_idx, val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# 5. Initialize Model\n",
    "# model = VQAModel(num_answers=num_classes).to(device)\n",
    "\n",
    "model = VQAModel(num_answers=num_classes) # Don't send to .to(device) yet\n",
    "\n",
    "# Check if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    # This wraps the model to split batches across both GPUs\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Now send the model to the primary device (cuda:0)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "# 6. Run Training\n",
    "num_epochs = 6 # Reduced for testing, increase later\n",
    "best_val_acc = 0.0\n",
    "\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_acc = validate(model, val_loader, device)\n",
    "    \n",
    "    # Clean print after the progress bars are done\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # torch.save(model.state_dict(), \"vqa_last.pth\")\n",
    "    torch.save(model.module.state_dict(), \"vqa_last.pth\")\n",
    "    print(\"saved last model\")\n",
    "    \n",
    "    # Save checkpoints\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        # torch.save(model.state_dict(), \"vqa_best.pth\")\n",
    "        torch.save(model.module.state_dict(), \"vqa_best.pth\")\n",
    "        print(\" -> Saved best model.\")\n",
    "    \n",
    "    !zip model.zip vqa_best.pth vqa_last.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-03T11:05:50.429449Z",
     "iopub.status.busy": "2026-01-03T11:05:50.429161Z",
     "iopub.status.idle": "2026-01-03T11:05:50.437153Z",
     "shell.execute_reply": "2026-01-03T11:05:50.436284Z",
     "shell.execute_reply.started": "2026-01-03T11:05:50.429423Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer_to_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/150847224.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 1. Save the file locally in the kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vocab.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 2. Generate a download link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'answer_to_idx' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# 1. Save the file locally in the kernel\n",
    "with open('vocab.json', 'w') as f:\n",
    "    json.dump(answer_to_idx, f)\n",
    "\n",
    "# 2. Generate a download link\n",
    "FileLink('vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 2938312,
     "sourceId": 5060663,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9148806,
     "sourceId": 14330200,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9151823,
     "sourceId": 14334423,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 143614612,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
